Index: train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\r\nimport sys \r\n# put the directory efficientvit instead of '..'\r\nsys.path.insert(1, os.path.join(sys.path[0], \"../efficientvit\"))\r\n# from efficientvit.models.utils import build_kwargs_from_config\r\nfrom efficientvit.models.efficientvit.seg import SegHead\r\n######\r\nimport torch\r\nimport pickle\r\nfrom model import TwinLite as net\r\nimport torch.backends.cudnn as cudnn\r\nimport DataSet as myDataLoader\r\nfrom argparse import ArgumentParser\r\nfrom utils import train, val, netParams, save_checkpoint, poly_lr_scheduler\r\nimport torch.optim.lr_scheduler\r\nfrom torchvision.transforms import transforms as T\r\n\r\n\r\n\r\n\r\nfrom efficientvit.seg_model_zoo import create_seg_model\r\n\r\nfrom loss import TotalLoss\r\nhead = SegHead(\r\n            fid_list=[\"stage4\", \"stage3\", \"stage2\"],\r\n            in_channel_list=[128, 64, 32],\r\n            stride_list=[64, 32, 16, 8],\r\n            head_stride=4,\r\n            head_width=32,\r\n            head_depth=1,\r\n            expand_ratio=4,\r\n            middle_op=\"mbconv\",\r\n            final_expand=4,\r\n            n_classes=2,\r\n#             **build_kwargs_from_config(kwargs, SegHead),\r\n        )\r\ndef train_net(args):\r\n    # load the model\r\n    cuda_available = torch.cuda.is_available()\r\n    num_gpus = torch.cuda.device_count()\r\n    # model = net.TwinLiteNet()\r\n    model = create_seg_model('b0','bdd',False)\r\n\r\n    if num_gpus > 1:\r\n        model = torch.nn.DataParallel(model)\r\n\r\n    args.savedir = args.savedir + '/'\r\n\r\n    # create the directory if not exist\r\n    if not os.path.exists(args.savedir):\r\n        os.mkdir(args.savedir)\r\n\r\n    transform=T.Compose([\r\n        T.ToTensor(),\r\n        T.Normalize(\r\n            mean=[0.485,0.456,0.406],\r\n            std=[0.229,0.224,0.225]\r\n        ),\r\n\r\n])\r\n    \r\n    trainLoader = torch.utils.data.DataLoader(\r\n        myDataLoader.MyDataset(transform=transform),\r\n        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\r\n\r\n\r\n    valLoader = torch.utils.data.DataLoader(\r\n        myDataLoader.MyDataset(valid=True),\r\n        batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)\r\n\r\n    if cuda_available:\r\n        args.onGPU = True\r\n        model = model.cuda()\r\n        cudnn.benchmark = True\r\n    # for param in model.parameters():\r\n    #\r\n    #     param.requires_grad = False\r\n    #     total_paramters = netParams(model)\r\n    #\r\n    # for param in model.head2.parameters():\r\n    #     param.requires_grad = True\r\n\r\n    # print('Total network parameters: ' + str(total_paramters))\r\n\r\n    start_epoch = 0\r\n    lr = args.lr\r\n\r\n    optimizer = torch.optim.Adam(model.parameters(), lr, (0.9, 0.999), eps=1e-08, weight_decay=5e-4)\r\n\r\n    if args.resume:\r\n        if os.path.isfile(args.resume):\r\n            print(\"=> loading checkpoint '{}'\".format(args.resume))\r\n            checkpoint = torch.load(args.resume)\r\n            start_epoch = checkpoint['epoch']\r\n            model.load_state_dict(checkpoint['state_dict'])\r\n            optimizer.load_state_dict(checkpoint['optimizer'])\r\n            print(\"=> loaded checkpoint '{}' (epoch {})\"\r\n                .format(args.resume, checkpoint['epoch']))\r\n        else:\r\n            print(\"=> no checkpoint found at '{}'\".format(args.resume))\r\n\r\n\r\n    for epoch in range(start_epoch, args.max_epochs):\r\n\r\n        model_file_name = args.savedir + os.sep + 'model_{}.pth'.format(epoch)\r\n        checkpoint_file_name = args.savedir + os.sep + 'checkpoint_{}.pth.tar'.format(epoch)\r\n        poly_lr_scheduler(args, optimizer, epoch)\r\n        for param_group in optimizer.param_groups:\r\n            lr = param_group['lr']\r\n        print(\"Learning rate: \" +  str(lr))\r\n\r\n        # train for one epoch\r\n        model.train()\r\n\r\n        # model.head1 = head\r\n        # model.head2 = head\r\n        # for param in model.backbone.parameters():\r\n        #     param.requires_grad = False\r\n\r\n        criteria = TotalLoss().cuda()\r\n\r\n        train(args, trainLoader, model, criteria, optimizer, epoch)\r\n        # model.eval()\r\n        # # validation\r\n        # val(valLoader, model)\r\n        torch.save(model.state_dict(), model_file_name)\r\n        \r\n        save_checkpoint({\r\n            'epoch': epoch + 1,\r\n            'state_dict': model.state_dict(),\r\n            'optimizer': optimizer.state_dict(),\r\n            'lr': lr\r\n        }, checkpoint_file_name)\r\n\r\n        \r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    parser = ArgumentParser()\r\n    parser.add_argument('--max_epochs', type=int, default=100, help='Max. number of epochs')\r\n    parser.add_argument('--num_workers', type=int, default=12, help='No. of parallel threads')\r\n    parser.add_argument('--batch_size', type=int, default=16, help='Batch size. 12 for ESPNet-C and 6 for ESPNet. '\r\n                                                                   'Change as per the GPU memory')\r\n    parser.add_argument('--step_loss', type=int, default=100, help='Decrease learning rate after how many epochs.')\r\n    parser.add_argument('--lr', type=float, default=5e-4, help='Initial learning rate')\r\n    parser.add_argument('--savedir', default='./test_', help='directory to save the results')\r\n    parser.add_argument('--resume', type=str, default='', help='Use this flag to load last checkpoint for training')\r\n    parser.add_argument('--pretrained', default='', help='Pretrained ESPNetv2 weights.')\r\n\r\n    train_net(parser.parse_args())\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train.py b/train.py
--- a/train.py	
+++ b/train.py	
@@ -112,10 +112,10 @@
         # train for one epoch
         model.train()
 
-        # model.head1 = head
-        # model.head2 = head
-        # for param in model.backbone.parameters():
-        #     param.requires_grad = False
+        model.head1 = head.cuda()
+        model.head2 = head.cuda()
+        for param in model.backbone.parameters():
+            param.requires_grad = False
 
         criteria = TotalLoss().cuda()
 
